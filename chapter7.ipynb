{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7b390b5",
   "metadata": {},
   "source": [
    "# Face analytics pipeline with G-API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b734cf4",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial you will learn:\n",
    "\n",
    "* Basics of a sample gaze estimation algorithm;\n",
    "* How to infer different networks inside a pipeline with G-API;\n",
    "* How to run a G-API pipeline on a video stream."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad68e64",
   "metadata": {},
   "source": [
    "## Pipeline overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fce2e16",
   "metadata": {},
   "source": [
    "\n",
    "Our example program is inspired by the \"Interactive Face Detection\" demonstration from OpenVINO™ Toolkit's Open Model Zoo. The streamlined process includes the following stages:\n",
    "\n",
    "* Capturing and decoding the image;\n",
    "* Preprocessed detection;\n",
    "* Preprocessed classification for each identified object using dual networks;\n",
    "* Displaying the results visually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff91a39d",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This sample requires:\n",
    "\n",
    "* PC with GNU/Linux or Microsoft Windows (Apple macOS is supported but was not tested);\n",
    "\n",
    "* OpenCV 4.2 or later built with Intel® Distribution of OpenVINO™ Toolkit (building with Intel® TBB is a plus);\n",
    "\n",
    "* The following topologies from OpenVINO™ Toolkit Open Model Zoo:\n",
    "    * face-detection-retail-0005\n",
    "    * head-pose-estimation-adas-0001\n",
    "    * facial-landmarks-35-adas-0002\n",
    "    * gaze-estimation-adas-0002\n",
    "    * open-closed-eye-0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3502aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5926e9",
   "metadata": {},
   "source": [
    "# Constructing a G-API pipeline\n",
    "\n",
    "## Define Custom Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f81c6abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------Support functions for custom kernels------------------------\n",
    "def intersection(surface, rect):\n",
    "    \"\"\" Remove zone of out of bound from ROI\n",
    "\n",
    "    Params:\n",
    "    surface: image bounds is rect representation (top left coordinates and width and height)\n",
    "    rect: region of interest is also has rect representation\n",
    "\n",
    "    Return:\n",
    "    Modified ROI with correct bounds\n",
    "    \"\"\"\n",
    "    l_x = max(surface[0], rect[0])\n",
    "    l_y = max(surface[1], rect[1])\n",
    "    width = min(surface[0] + surface[2], rect[0] + rect[2]) - l_x\n",
    "    height = min(surface[1] + surface[3], rect[1] + rect[3]) - l_y\n",
    "    if width < 0 or height < 0:\n",
    "        return (0, 0, 0, 0)\n",
    "    return (l_x, l_y, width, height)\n",
    "\n",
    "\n",
    "def process_landmarks(r_x, r_y, r_w, r_h, landmarks):\n",
    "    \"\"\" Create points from result of inference of facial-landmarks network and size of input image\n",
    "\n",
    "    Params:\n",
    "    r_x: x coordinate of top left corner of input image\n",
    "    r_y: y coordinate of top left corner of input image\n",
    "    r_w: width of input image\n",
    "    r_h: height of input image\n",
    "    landmarks: result of inference of facial-landmarks network\n",
    "\n",
    "    Return:\n",
    "    Array of landmarks points for one face\n",
    "    \"\"\"\n",
    "    lmrks = landmarks[0]\n",
    "    raw_x = lmrks[::2] * r_w + r_x\n",
    "    raw_y = lmrks[1::2] * r_h + r_y\n",
    "    return np.array([[int(x), int(y)] for x, y in zip(raw_x, raw_y)])\n",
    "\n",
    "\n",
    "def eye_box(p_1, p_2, scale=1.8):\n",
    "    \"\"\" Get bounding box of eye\n",
    "\n",
    "    Params:\n",
    "    p_1: point of left edge of eye\n",
    "    p_2: point of right edge of eye\n",
    "    scale: change size of box with this value\n",
    "\n",
    "    Return:\n",
    "    Bounding box of eye and its midpoint\n",
    "    \"\"\"\n",
    "\n",
    "    size = np.linalg.norm(p_1 - p_2)\n",
    "    midpoint = (p_1 + p_2) / 2\n",
    "    width = scale * size\n",
    "    height = width\n",
    "    p_x = midpoint[0] - (width / 2)\n",
    "    p_y = midpoint[1] - (height / 2)\n",
    "    return (int(p_x), int(p_y), int(width), int(height)), list(map(int, midpoint))\n",
    "\n",
    "\n",
    "# ------------------------Custom graph operations------------------------\n",
    "@cv.gapi.op('custom.GProcessPoses',\n",
    "            in_types=[cv.GArray.GMat, cv.GArray.GMat, cv.GArray.GMat],\n",
    "            out_types=[cv.GArray.GMat])\n",
    "class GProcessPoses:\n",
    "    \"\"\"\n",
    "    Custom G-API operation to process head poses.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def outMeta(arr_desc0, arr_desc1, arr_desc2):\n",
    "        return cv.empty_array_desc()\n",
    "\n",
    "\n",
    "@cv.gapi.op('custom.GParseEyes',\n",
    "            in_types=[cv.GArray.GMat, cv.GArray.Rect, cv.GOpaque.Size],\n",
    "            out_types=[cv.GArray.Rect, cv.GArray.Rect, cv.GArray.Point, cv.GArray.Point])\n",
    "class GParseEyes:\n",
    "    \"\"\"\n",
    "    Custom G-API operation to parse eye information.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def outMeta(arr_desc0, arr_desc1, arr_desc2):\n",
    "        return cv.empty_array_desc(), cv.empty_array_desc(), \\\n",
    "               cv.empty_array_desc(), cv.empty_array_desc()\n",
    "\n",
    "\n",
    "@cv.gapi.op('custom.GGetStates',\n",
    "            in_types=[cv.GArray.GMat, cv.GArray.GMat],\n",
    "            out_types=[cv.GArray.Int, cv.GArray.Int])\n",
    "class GGetStates:\n",
    "    \"\"\"\n",
    "    Custom G-API operation to get the state of each eye (open or closed).\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def outMeta(arr_desc0, arr_desc1):\n",
    "        return cv.empty_array_desc(), cv.empty_array_desc()\n",
    "\n",
    "\n",
    "# ------------------------Custom kernels------------------------\n",
    "@cv.gapi.kernel(GProcessPoses)\n",
    "class GProcessPosesImpl:\n",
    "    \"\"\" Custom kernel. Processed poses of heads\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def run(in_ys, in_ps, in_rs):\n",
    "        \"\"\" Сustom kernel executable code\n",
    "\n",
    "        Params:\n",
    "        in_ys: yaw angle of head\n",
    "        in_ps: pitch angle of head\n",
    "        in_rs: roll angle of head\n",
    "\n",
    "        Return:\n",
    "        Arrays with heads poses\n",
    "        \"\"\"\n",
    "        return [np.array([ys[0], ps[0], rs[0]]).T for ys, ps, rs in zip(in_ys, in_ps, in_rs)]\n",
    "\n",
    "\n",
    "@cv.gapi.kernel(GParseEyes)\n",
    "class GParseEyesImpl:\n",
    "    \"\"\" Custom kernel. Get information about eyes\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def run(in_landm_per_face, in_face_rcs, frame_size):\n",
    "        \"\"\" Сustom kernel executable code\n",
    "\n",
    "        Params:\n",
    "        in_landm_per_face: landmarks from inference of facial-landmarks network for each face\n",
    "        in_face_rcs: bounding boxes for each face\n",
    "        frame_size: size of input image\n",
    "\n",
    "        Return:\n",
    "        Arrays of ROI for left and right eyes, array of midpoints and\n",
    "        array of landmarks points\n",
    "        \"\"\"\n",
    "        left_eyes = []\n",
    "        right_eyes = []\n",
    "        midpoints = []\n",
    "        lmarks = []\n",
    "        surface = (0, 0, *frame_size)\n",
    "        for landm_face, rect in zip(in_landm_per_face, in_face_rcs):\n",
    "            points = process_landmarks(*rect, landm_face)\n",
    "            lmarks.extend(points)\n",
    "\n",
    "            rect, midpoint_l = eye_box(points[0], points[1])\n",
    "            left_eyes.append(intersection(surface, rect))\n",
    "\n",
    "            rect, midpoint_r = eye_box(points[2], points[3])\n",
    "            right_eyes.append(intersection(surface, rect))\n",
    "\n",
    "            midpoints.append(midpoint_l)\n",
    "            midpoints.append(midpoint_r)\n",
    "        return left_eyes, right_eyes, midpoints, lmarks\n",
    "\n",
    "\n",
    "@cv.gapi.kernel(GGetStates)\n",
    "class GGetStatesImpl:\n",
    "    \"\"\" Custom kernel. Get state of eye - open or closed\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def run(eyesl, eyesr):\n",
    "        \"\"\" Сustom kernel executable code\n",
    "\n",
    "        Params:\n",
    "        eyesl: result of inference of open-closed-eye network for left eye\n",
    "        eyesr: result of inference of open-closed-eye network for right eye\n",
    "\n",
    "        Return:\n",
    "        States of left eyes and states of right eyes\n",
    "        \"\"\"\n",
    "        out_l_st = [int(st) for eye_l in eyesl for st in (eye_l[:, 0] < eye_l[:, 1]).ravel()]\n",
    "        out_r_st = [int(st) for eye_r in eyesr for st in (eye_r[:, 0] < eye_r[:, 1]).ravel()]\n",
    "        return out_l_st, out_r_st\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa7addd",
   "metadata": {},
   "source": [
    "## Building a GComputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73232978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------Demo's graph------------------------\n",
    "\"\"\"\n",
    "    Creates a graph for a demo application, handling various types of computer vision tasks.\n",
    "    This includes face detection, head pose estimation, facial landmark detection, \n",
    "    eye state estimation, and gaze estimation.\n",
    "    \n",
    "    Returns:\n",
    "    A GComputation object that represents the constructed graph.\n",
    "    \"\"\"\n",
    "g_in = cv.GMat()\n",
    "\n",
    "# Detect faces\n",
    "face_inputs = cv.GInferInputs()\n",
    "face_inputs.setInput('data', g_in)\n",
    "face_outputs = cv.gapi.infer('face-detection', face_inputs)\n",
    "faces = face_outputs.at('detection_out')\n",
    "\n",
    "# Parse faces\n",
    "sz = cv.gapi.streaming.size(g_in)\n",
    "faces_rc = cv.gapi.parseSSD(faces, sz, 0.5, False, False)\n",
    "\n",
    "# Detect poses\n",
    "head_inputs = cv.GInferInputs()\n",
    "head_inputs.setInput('data', g_in)\n",
    "face_outputs = cv.gapi.infer('head-pose', faces_rc, head_inputs)\n",
    "angles_y = face_outputs.at('angle_y_fc')\n",
    "angles_p = face_outputs.at('angle_p_fc')\n",
    "angles_r = face_outputs.at('angle_r_fc')\n",
    "\n",
    "# Parse poses\n",
    "heads_pos = GProcessPoses.on(angles_y, angles_p, angles_r)\n",
    "\n",
    "# Detect landmarks\n",
    "landmark_inputs = cv.GInferInputs()\n",
    "landmark_inputs.setInput('data', g_in)\n",
    "landmark_outputs = cv.gapi.infer('facial-landmarks', faces_rc,\n",
    "                                 landmark_inputs)\n",
    "landmark = landmark_outputs.at('align_fc3')\n",
    "\n",
    "# Parse landmarks\n",
    "left_eyes, right_eyes, mids, lmarks = GParseEyes.on(landmark, faces_rc, sz)\n",
    "\n",
    "# Detect eyes\n",
    "eyes_inputs = cv.GInferInputs()\n",
    "eyes_inputs.setInput('input.1', g_in)\n",
    "eyesl_outputs = cv.gapi.infer('open-closed-eye', left_eyes, eyes_inputs)\n",
    "eyesr_outputs = cv.gapi.infer('open-closed-eye', right_eyes, eyes_inputs)\n",
    "eyesl = eyesl_outputs.at('19')\n",
    "eyesr = eyesr_outputs.at('19')\n",
    "\n",
    "# Process eyes states\n",
    "l_eye_st, r_eye_st = GGetStates.on(eyesl, eyesr)\n",
    "\n",
    "# Gaze estimation\n",
    "gaze_inputs = cv.GInferListInputs()\n",
    "gaze_inputs.setInput('left_eye_image', left_eyes)\n",
    "gaze_inputs.setInput('right_eye_image', right_eyes)\n",
    "gaze_inputs.setInput('head_pose_angles', heads_pos)\n",
    "gaze_outputs = cv.gapi.infer2('gaze-estimation', g_in, gaze_inputs)\n",
    "gaze_vectors = gaze_outputs.at('gaze_vector')\n",
    "\n",
    "out = cv.gapi.copy(g_in)\n",
    "# ------------------------End of graph------------------------\n",
    "\n",
    "comp = cv.GComputation(cv.GIn(g_in), cv.GOut(out,\n",
    "                                             faces_rc,\n",
    "                                             left_eyes,\n",
    "                                             right_eyes,\n",
    "                                             gaze_vectors,\n",
    "                                             angles_y,\n",
    "                                             angles_p,\n",
    "                                             angles_r,\n",
    "                                             l_eye_st,\n",
    "                                             r_eye_st,\n",
    "                                             mids,\n",
    "                                             lmarks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aeebd6",
   "metadata": {},
   "source": [
    "# Configuring the pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fe46044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Networks\n",
    "face_net = cv.gapi.ie.params('face-detection', 'face-detection-retail-0005.xml',\n",
    "                             'face-detection-retail-0005.bin', 'CPU')\n",
    "head_pose_net = cv.gapi.ie.params('head-pose', 'head-pose-estimation-adas-0001.xml',\n",
    "                                  'head-pose-estimation-adas-0001.bin', 'CPU')\n",
    "landmarks_net = cv.gapi.ie.params('facial-landmarks', 'facial-landmarks-35-adas-0002.xml',\n",
    "                                  'facial-landmarks-35-adas-0002.xml', 'CPU')\n",
    "gaze_net = cv.gapi.ie.params('gaze-estimation', 'gaze-estimation-adas-0002.xml',\n",
    "                             'gaze-estimation-adas-0002.bin', 'CPU')\n",
    "eye_net = cv.gapi.ie.params('open-closed-eye', 'open-closed-eye-0001.xml',\n",
    "                            'open-closed-eye-0001.bin', 'CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e714142",
   "metadata": {},
   "source": [
    "Once networks are defined and custom kernels are implemented, the pipeline is compiled for streaming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8110a356",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Kernels pack\n",
    "kernels = cv.gapi.kernels(GParseEyesImpl, GProcessPosesImpl, GGetStatesImpl)\n",
    "\n",
    "# Network\n",
    "nets = cv.gapi.networks(face_net, head_pose_net, landmarks_net, gaze_net, eye_net)\n",
    "\n",
    "# Compile our pipeline and pass our kernels & networks as\n",
    "# parameters. This is the place where G-API learns which\n",
    "# networks & kernels we're actually operating with (the graph\n",
    "# description itself known nothing about that).\n",
    "ccomp = comp.compileStreaming(args=cv.gapi.compile_args(kernels, nets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de21369",
   "metadata": {},
   "source": [
    "# Running the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c52f626",
   "metadata": {},
   "outputs": [],
   "source": [
    "ccomp = comp.compileStreaming(args=cv.gapi.compile_args(kernels, nets))\n",
    "source = cv.gapi.wip.make_capture_src(input(\"Please Input the Video Path.\"))\n",
    "ccomp.setSource(cv.gin(source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0994401",
   "metadata": {},
   "outputs": [],
   "source": [
    "ccomp.start()\n",
    "\n",
    "frames = 0\n",
    "fps = 0\n",
    "print('Processing')\n",
    "START_TIME = time.time()\n",
    "\n",
    "while True:\n",
    "    start_time_cycle = time.time()\n",
    "    has_frame, (oimg,\n",
    "                outr,\n",
    "                l_eyes,\n",
    "                r_eyes,\n",
    "                outg,\n",
    "                out_y,\n",
    "                out_p,\n",
    "                out_r,\n",
    "                out_st_l,\n",
    "                out_st_r,\n",
    "                out_mids,\n",
    "                outl) = ccomp.pull()\n",
    "\n",
    "    if not has_frame:\n",
    "        break\n",
    "\n",
    "    # Draw\n",
    "    GREEN = (0, 255, 0)\n",
    "    RED = (0, 0, 255)\n",
    "    WHITE = (255, 255, 255)\n",
    "    BLUE = (255, 0, 0)\n",
    "    PINK = (255, 0, 255)\n",
    "    YELLOW = (0, 255, 255)\n",
    "\n",
    "    M_PI_180 = np.pi / 180\n",
    "    M_PI_2 = np.pi / 2\n",
    "    M_PI = np.pi\n",
    "\n",
    "    FACES_SIZE = len(outr)\n",
    "\n",
    "    for i, out_rect in enumerate(outr):\n",
    "        # Face box\n",
    "        cv.rectangle(oimg, out_rect, WHITE, 1)\n",
    "        rx, ry, rwidth, rheight = out_rect\n",
    "\n",
    "        # Landmarks\n",
    "        lm_radius = int(0.01 * rwidth + 1)\n",
    "        lmsize = int(len(outl) / FACES_SIZE)\n",
    "        for j in range(lmsize):\n",
    "            cv.circle(oimg, outl[j + i * lmsize], lm_radius, YELLOW, -1)\n",
    "\n",
    "        # Headposes\n",
    "        yaw = out_y[i]\n",
    "        pitch = out_p[i]\n",
    "        roll = out_r[i]\n",
    "        sin_y = np.sin(yaw[:] * M_PI_180)\n",
    "        sin_p = np.sin(pitch[:] * M_PI_180)\n",
    "        sin_r = np.sin(roll[:] * M_PI_180)\n",
    "\n",
    "        cos_y = np.cos(yaw[:] * M_PI_180)\n",
    "        cos_p = np.cos(pitch[:] * M_PI_180)\n",
    "        cos_r = np.cos(roll[:] * M_PI_180)\n",
    "\n",
    "        axis_length = 0.4 * rwidth\n",
    "        x_center = int(rx + rwidth / 2)\n",
    "        y_center = int(ry + rheight / 2)\n",
    "\n",
    "        # center to right\n",
    "        cv.line(oimg, [x_center, y_center],\n",
    "                [int(x_center + axis_length * (cos_r * cos_y + sin_y * sin_p * sin_r)),\n",
    "                 int(y_center + axis_length * cos_p * sin_r)],\n",
    "                RED, 2)\n",
    "\n",
    "        # center to top\n",
    "        cv.line(oimg, [x_center, y_center],\n",
    "                [int(x_center + axis_length * (cos_r * sin_y * sin_p + cos_y * sin_r)),\n",
    "                 int(y_center - axis_length * cos_p * cos_r)],\n",
    "                GREEN, 2)\n",
    "\n",
    "        # center to forward\n",
    "        cv.line(oimg, [x_center, y_center],\n",
    "                [int(x_center + axis_length * sin_y * cos_p),\n",
    "                 int(y_center + axis_length * sin_p)],\n",
    "                PINK, 2)\n",
    "\n",
    "        scale_box = 0.002 * rwidth\n",
    "        cv.putText(oimg, \"head pose: (y=%0.0f, p=%0.0f, r=%0.0f)\" %\n",
    "                   (np.round(yaw), np.round(pitch), np.round(roll)),\n",
    "                   [int(rx), int(ry + rheight + 5 * rwidth / 100)],\n",
    "                   cv.FONT_HERSHEY_PLAIN, scale_box * 2, WHITE, 1)\n",
    "\n",
    "        # Eyes boxes\n",
    "        color_l = GREEN if out_st_l[i] else RED\n",
    "        cv.rectangle(oimg, l_eyes[i], color_l, 1)\n",
    "        color_r = GREEN if out_st_r[i] else RED\n",
    "        cv.rectangle(oimg, r_eyes[i], color_r, 1)\n",
    "\n",
    "        # Gaze vectors\n",
    "        norm_gazes = np.linalg.norm(outg[i][0])\n",
    "        gaze_vector = outg[i][0] / norm_gazes\n",
    "\n",
    "        arrow_length = 0.4 * rwidth\n",
    "        gaze_arrow = [arrow_length * gaze_vector[0], -arrow_length * gaze_vector[1]]\n",
    "        left_arrow = [int(a+b) for a, b in zip(out_mids[0 + i * 2], gaze_arrow)]\n",
    "        right_arrow = [int(a+b) for a, b in zip(out_mids[1 + i * 2], gaze_arrow)]\n",
    "        if out_st_l[i]:\n",
    "            cv.arrowedLine(oimg, out_mids[0 + i * 2], left_arrow, BLUE, 2)\n",
    "        if out_st_r[i]:\n",
    "            cv.arrowedLine(oimg, out_mids[1 + i * 2], right_arrow, BLUE, 2)\n",
    "\n",
    "        v0, v1, v2 = outg[i][0]\n",
    "\n",
    "        gaze_angles = [180 / M_PI * (M_PI_2 + np.arctan2(v2, v0)),\n",
    "                       180 / M_PI * (M_PI_2 - np.arccos(v1 / norm_gazes))]\n",
    "        cv.putText(oimg, \"gaze angles: (h=%0.0f, v=%0.0f)\" %\n",
    "                   (np.round(gaze_angles[0]), np.round(gaze_angles[1])),\n",
    "                   [int(rx), int(ry + rheight + 12 * rwidth / 100)],\n",
    "                   cv.FONT_HERSHEY_PLAIN, scale_box * 2, WHITE, 1)\n",
    "\n",
    "    # Add FPS value to frame\n",
    "    cv.putText(oimg, \"FPS: %0i\" % (fps), [int(20), int(40)],\n",
    "               cv.FONT_HERSHEY_PLAIN, 2, RED, 2)\n",
    "\n",
    "    # Show result\n",
    "    cv.imshow('Gaze Estimation', oimg)\n",
    "    cv.waitKey(1)\n",
    "\n",
    "    fps = int(1. / (time.time() - start_time_cycle))\n",
    "    frames += 1\n",
    "EXECUTION_TIME = time.time() - START_TIME\n",
    "print('Execution successful')\n",
    "print('Mean FPS is ', int(frames / EXECUTION_TIME))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c1f680",
   "metadata": {},
   "source": [
    "# Comparison with serial mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dddf827",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = cv.gapi.wip.make_capture_src(input(\"Please Input the Camera Device Number.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e5f25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ccomp.start()\n",
    "\n",
    "frames = 0\n",
    "fps = 0\n",
    "print('Processing')\n",
    "START_TIME = time.time()\n",
    "\n",
    "while True:\n",
    "    start_time_cycle = time.time()\n",
    "    has_frame, (oimg,\n",
    "                outr,\n",
    "                l_eyes,\n",
    "                r_eyes,\n",
    "                outg,\n",
    "                out_y,\n",
    "                out_p,\n",
    "                out_r,\n",
    "                out_st_l,\n",
    "                out_st_r,\n",
    "                out_mids,\n",
    "                outl) = ccomp.pull()\n",
    "\n",
    "    if not has_frame:\n",
    "        break\n",
    "\n",
    "    # Draw\n",
    "    GREEN = (0, 255, 0)\n",
    "    RED = (0, 0, 255)\n",
    "    WHITE = (255, 255, 255)\n",
    "    BLUE = (255, 0, 0)\n",
    "    PINK = (255, 0, 255)\n",
    "    YELLOW = (0, 255, 255)\n",
    "\n",
    "    M_PI_180 = np.pi / 180\n",
    "    M_PI_2 = np.pi / 2\n",
    "    M_PI = np.pi\n",
    "\n",
    "    FACES_SIZE = len(outr)\n",
    "\n",
    "    for i, out_rect in enumerate(outr):\n",
    "        # Face box\n",
    "        cv.rectangle(oimg, out_rect, WHITE, 1)\n",
    "        rx, ry, rwidth, rheight = out_rect\n",
    "\n",
    "        # Landmarks\n",
    "        lm_radius = int(0.01 * rwidth + 1)\n",
    "        lmsize = int(len(outl) / FACES_SIZE)\n",
    "        for j in range(lmsize):\n",
    "            cv.circle(oimg, outl[j + i * lmsize], lm_radius, YELLOW, -1)\n",
    "\n",
    "        # Headposes\n",
    "        yaw = out_y[i]\n",
    "        pitch = out_p[i]\n",
    "        roll = out_r[i]\n",
    "        sin_y = np.sin(yaw[:] * M_PI_180)\n",
    "        sin_p = np.sin(pitch[:] * M_PI_180)\n",
    "        sin_r = np.sin(roll[:] * M_PI_180)\n",
    "\n",
    "        cos_y = np.cos(yaw[:] * M_PI_180)\n",
    "        cos_p = np.cos(pitch[:] * M_PI_180)\n",
    "        cos_r = np.cos(roll[:] * M_PI_180)\n",
    "\n",
    "        axis_length = 0.4 * rwidth\n",
    "        x_center = int(rx + rwidth / 2)\n",
    "        y_center = int(ry + rheight / 2)\n",
    "\n",
    "        # center to right\n",
    "        cv.line(oimg, [x_center, y_center],\n",
    "                [int(x_center + axis_length * (cos_r * cos_y + sin_y * sin_p * sin_r)),\n",
    "                 int(y_center + axis_length * cos_p * sin_r)],\n",
    "                RED, 2)\n",
    "\n",
    "        # center to top\n",
    "        cv.line(oimg, [x_center, y_center],\n",
    "                [int(x_center + axis_length * (cos_r * sin_y * sin_p + cos_y * sin_r)),\n",
    "                 int(y_center - axis_length * cos_p * cos_r)],\n",
    "                GREEN, 2)\n",
    "\n",
    "        # center to forward\n",
    "        cv.line(oimg, [x_center, y_center],\n",
    "                [int(x_center + axis_length * sin_y * cos_p),\n",
    "                 int(y_center + axis_length * sin_p)],\n",
    "                PINK, 2)\n",
    "\n",
    "        scale_box = 0.002 * rwidth\n",
    "        cv.putText(oimg, \"head pose: (y=%0.0f, p=%0.0f, r=%0.0f)\" %\n",
    "                   (np.round(yaw), np.round(pitch), np.round(roll)),\n",
    "                   [int(rx), int(ry + rheight + 5 * rwidth / 100)],\n",
    "                   cv.FONT_HERSHEY_PLAIN, scale_box * 2, WHITE, 1)\n",
    "\n",
    "        # Eyes boxes\n",
    "        color_l = GREEN if out_st_l[i] else RED\n",
    "        cv.rectangle(oimg, l_eyes[i], color_l, 1)\n",
    "        color_r = GREEN if out_st_r[i] else RED\n",
    "        cv.rectangle(oimg, r_eyes[i], color_r, 1)\n",
    "\n",
    "        # Gaze vectors\n",
    "        norm_gazes = np.linalg.norm(outg[i][0])\n",
    "        gaze_vector = outg[i][0] / norm_gazes\n",
    "\n",
    "        arrow_length = 0.4 * rwidth\n",
    "        gaze_arrow = [arrow_length * gaze_vector[0], -arrow_length * gaze_vector[1]]\n",
    "        left_arrow = [int(a+b) for a, b in zip(out_mids[0 + i * 2], gaze_arrow)]\n",
    "        right_arrow = [int(a+b) for a, b in zip(out_mids[1 + i * 2], gaze_arrow)]\n",
    "        if out_st_l[i]:\n",
    "            cv.arrowedLine(oimg, out_mids[0 + i * 2], left_arrow, BLUE, 2)\n",
    "        if out_st_r[i]:\n",
    "            cv.arrowedLine(oimg, out_mids[1 + i * 2], right_arrow, BLUE, 2)\n",
    "\n",
    "        v0, v1, v2 = outg[i][0]\n",
    "\n",
    "        gaze_angles = [180 / M_PI * (M_PI_2 + np.arctan2(v2, v0)),\n",
    "                       180 / M_PI * (M_PI_2 - np.arccos(v1 / norm_gazes))]\n",
    "        cv.putText(oimg, \"gaze angles: (h=%0.0f, v=%0.0f)\" %\n",
    "                   (np.round(gaze_angles[0]), np.round(gaze_angles[1])),\n",
    "                   [int(rx), int(ry + rheight + 12 * rwidth / 100)],\n",
    "                   cv.FONT_HERSHEY_PLAIN, scale_box * 2, WHITE, 1)\n",
    "\n",
    "    # Add FPS value to frame\n",
    "    cv.putText(oimg, \"FPS: %0i\" % (fps), [int(20), int(40)],\n",
    "               cv.FONT_HERSHEY_PLAIN, 2, RED, 2)\n",
    "\n",
    "    # Show result\n",
    "    cv.imshow('Gaze Estimation', oimg)\n",
    "    cv.waitKey(1)\n",
    "\n",
    "    fps = int(1. / (time.time() - start_time_cycle))\n",
    "    frames += 1\n",
    "EXECUTION_TIME = time.time() - START_TIME\n",
    "print('Execution successful')\n",
    "print('Mean FPS is ', int(frames / EXECUTION_TIME))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a43c8e",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e89a85b",
   "metadata": {},
   "source": [
    "G-API provides an innovative approach to constructing and optimizing pipelines that incorporate both traditional and machine learning algorithms. Transitioning to a different execution paradigm doesn't necessitate alterations to the algorithmic code written using G-API; the only modification needed is in how the graph is invoked.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
